{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845602a-d31a-4766-b6e4-18bcedef967e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4601d9a5-e5f1-4bf3-8e17-3caccc3d8014",
   "metadata": {},
   "source": [
    "This code trains a modified GPT-2 model using Proximal Policy Optimization (PPO) to generate text that maximizes a custom reward function. Here's an overview of the main components:\n",
    "\n",
    "TextGenerationEnvironment class: Defines an environment for generating text using GPT-2. It provides functions to generate text, reset the environment, and perform actions (generate tokens).\n",
    "\n",
    "ModifiedGPT class: A modified GPT-2 model with an additional linear layer to compute attention weights. The model computes action probabilities using the softmax function.\n",
    "\n",
    "RewardFunction class: Defines a custom reward function that computes rewards based on the generated text's perplexity compared to a reference text. A lower perplexity corresponds to a higher reward.\n",
    "\n",
    "RolloutStorage class: Stores the observations, actions, action probabilities, rewards, and done flags during a rollout. It also computes the return and advantage values used in PPO training.\n",
    "\n",
    "PPOTrainer class: Trains the ModifiedGPT model using the PPO algorithm. It takes an environment, a model, and a reward function as inputs and optimizes the model's parameters. The trainer performs multiple epochs of training on the collected rollouts.\n",
    "\n",
    "main function: Sets up the environment, model, reward function, and trainer. It trains the model for a specified number of steps and saves the trained model.\n",
    "\n",
    "Testing code snippet: Loads the fine-tuned model and generates text using a given prompt. It demonstrates how to use the modified GPT-2 model to generate text.\n",
    "\n",
    "The training process involves generating text in the environment and optimizing the model's parameters to maximize the custom reward function. The modified GPT-2 model is fine-tuned using the Proximal Policy Optimization algorithm, which is particularly suitable for environments with sparse rewards like text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefc614d-d974-494a-bed0-ecd87da258f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village, a young man named Tzal, who had been a member of the village's council, was killed by a group of bandits. He was taken to the village of Tzal, where he\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "    \n",
    "class TextGenerationEnvironment:\n",
    "    def __init__(self, model_name_or_path, max_length=20):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "        self.max_length = max_length\n",
    "        self.current_text = \"\"\n",
    "        self.current_length = 0\n",
    "\n",
    "    def generate_text(self, input_text, max_length=None):\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt')\n",
    "        output = self.model.generate(input_ids=input_ids, max_length=max_length)\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_text = \"\"\n",
    "        self.current_length = 0\n",
    "        return \"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        action_token = self.tokenizer.decode([action])\n",
    "        self.current_text += action_token\n",
    "\n",
    "        obs = self.current_text[-self.max_length:]\n",
    "        obs = obs if len(obs) > 0 else ' '  # Ensure the observation is never empty\n",
    "\n",
    "        reward = self.reward_fn(self.current_text)\n",
    "        self.current_length += 1\n",
    "        done = self.current_length >= self.max_length\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "class ModifiedGPT(nn.Module):\n",
    "    def __init__(self, model_name_or_path, num_actions=512):\n",
    "        super(ModifiedGPT, self).__init__()\n",
    "        config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=config)\n",
    "        self.action_layer = nn.Linear(config.n_embd, num_actions)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        gpt_outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = gpt_outputs[0]\n",
    "        # Calculate attention weights using the additional component\n",
    "        action = self.action_layer(hidden_states)\n",
    "        action_probs = self.softmax(action)\n",
    "\n",
    "        outputs = (action_probs,) + gpt_outputs[1:]\n",
    "\n",
    "        if labels is not None:\n",
    "            # Calculate the loss with the labels provided\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(outputs[0].view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class RewardFunction:\n",
    "    def __init__(self, reference_text, model_name_or_path):\n",
    "        self.reference_tokens = GPT2Tokenizer.from_pretrained(model_name_or_path).encode(reference_text, return_tensors='pt')\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "    def __call__(self, generated_text):\n",
    "        generated_tokens = GPT2Tokenizer.from_pretrained(model_name_or_path).encode(generated_text, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=generated_tokens, labels=generated_tokens)\n",
    "            loss = outputs[0]\n",
    "\n",
    "        perplexity = torch.exp(loss)\n",
    "        reward = -perplexity.item()\n",
    "        return reward\n",
    "    \n",
    "class RolloutStorage:\n",
    "    def __init__(self, max_length):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def store(self, obs, action, action_probs, reward, done):\n",
    "        self.observations.append(obs)\n",
    "        self.actions.append(action)\n",
    "        self.action_probs.append(action_probs)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def store_last_observation(self, value):\n",
    "        self.observations.append(value)\n",
    "\n",
    "    def clear(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "    def compute_returns(self, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r, done in zip(reversed(self.rewards), reversed(self.dones)):\n",
    "            if done:\n",
    "                R = 0\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def compute_advantages(self, returns):\n",
    "        advantages = []\n",
    "        for action_prob, return_ in zip(self.action_probs, returns):\n",
    "            advantages.append(return_ - action_prob)\n",
    "        return advantages\n",
    "\n",
    "    def batch_by_indices(self, indices):\n",
    "        obs_batch = [self.observations[i] for i in indices]\n",
    "        action_batch = [self.actions[i] for i in indices]\n",
    "        action_prob_batch = [self.action_probs[i] for i in indices]\n",
    "        advantage_batch = [self.advantages[i] for i in indices]\n",
    "        return_batch = [self.returns[i] for i in indices]\n",
    "        return obs_batch, action_batch, action_prob_batch, advantage_batch, return_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "class PPOTrainer:\n",
    "    \n",
    "    def __init__(self, env, model, reward_fn, lr=1e-4, betas=(0.9, 0.999), eps=1e-5, gamma=0.99, clip_param=0.2, value_loss_coef=0.5, entropy_coef=0.01, num_epochs=10, batch_size=64):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.reward_fn = reward_fn\n",
    "        self.gamma = gamma\n",
    "        self.clip_param = clip_param\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.num_epochs = num_epochs  # Add this line\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, eps=eps)\n",
    "    def train(self, num_steps):\n",
    "        storage = RolloutStorage(self.env.max_length)\n",
    "        obs = self.env.reset()\n",
    "    \n",
    "        done = True  # Add this line to initialize the 'done' variable\n",
    "    \n",
    "        for step in range(num_steps):\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "        \n",
    "            for t in range(self.env.max_length):\n",
    "                obs_tensor = torch.tensor(self.env.tokenizer.encode(obs), dtype=torch.long).unsqueeze(0)\n",
    "                if obs_tensor.shape[-1] == 0:\n",
    "                    continue\n",
    "                    \n",
    "                action_probs_tensor, value_tensor = self.model(obs_tensor)\n",
    "                action_probs = action_probs_tensor.squeeze(0).detach().numpy()\n",
    "                action = np.random.choice(len(action_probs), p=action_probs)\n",
    "\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                storage.store(obs, action, action_probs, reward, done)\n",
    "            \n",
    "                obs = next_obs\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                obs_tensor = torch.tensor(self.env.tokenizer.encode(obs), dtype=torch.long).unsqueeze(0)\n",
    "                _, value_tensor = self.model(obs_tensor)\n",
    "                storage.store_last_observation(value_tensor)\n",
    "            else:\n",
    "                storage.store_last_observation(torch.tensor(0.0))\n",
    "\n",
    "            returns = storage.compute_returns(self.gamma)\n",
    "            advantages = storage.compute_advantages(returns)\n",
    "    \n",
    "            for _ in range(self.num_epochs):\n",
    "                indices = np.arange(len(storage))\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "                for batch_start in range(0, len(storage), self.batch_size):\n",
    "                    batch_indices = indices[batch_start: batch_start + self.batch_size]\n",
    "                    obs_batch, action_batch, action_prob_batch, advantage_batch, return_batch = storage.batch_by_indices(batch_indices)\n",
    "\n",
    "                    self.update(obs_batch, action_batch, action_prob_batch, advantage_batch, return_batch)\n",
    "\n",
    "            storage.clear()\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = TextGenerationEnvironment(model_name_or_path='gpt2', max_length=20)\n",
    "    model = ModifiedGPT(model_name_or_path='gpt2', num_actions=512)\n",
    "    reward_fn = RewardFunction(reference_text=\"The quick brown fox jumps over the lazy dog\", model_name_or_path='gpt2')\n",
    "    trainer = PPOTrainer(env, model, reward_fn, lr=1e-4, betas=(0.9, 0.999), eps=1e-5, gamma=0.99, clip_param=0.2, value_loss_coef=0.5, entropy_coef=0.01)\n",
    "\n",
    "    num_steps = 10000\n",
    "    trainer.train(num_steps)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'modified_gpt_model.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "    \n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, length=500):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.gpt.generate(input_ids=input_ids, max_length=length)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Load the pre-trained GPT-2 model\n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Create an instance of ModifiedGPT with the required components\n",
    "    model = ModifiedGPT(\"gpt2\")\n",
    "\n",
    "    # Load the fine-tuned model weights\n",
    "    model.load_state_dict(torch.load(\"modified_gpt_model.pth\"))\n",
    "\n",
    "    prompt = \"Once upon a time, in a small village\"\n",
    "    generated_text = generate_text(prompt, model, tokenizer, length=50)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3954d-71a1-4671-970c-525526ac6a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d88c540-c6b5-42f7-be7a-0ba09152c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "egypt is a very important part of the world.\n",
      "\n",
      "The first time I saw the film was in the early 1990s, when I was working on a documentary about the history of the Egyptian pyramids. I was working on a documentary about\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModifiedGPT(nn.Module):\n",
    "    def __init__(self, model_name_or_path, num_actions=512):\n",
    "        super(ModifiedGPT, self).__init__()\n",
    "        config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=config)\n",
    "        self.action_layer = nn.Linear(config.n_embd, num_actions)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        gpt_outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = gpt_outputs[0]\n",
    "        # Calculate attention weights using the additional component\n",
    "        action = self.action_layer(hidden_states)\n",
    "        action_probs = self.softmax(action)\n",
    "\n",
    "        outputs = (action_probs,) + gpt_outputs[1:]\n",
    "\n",
    "        if labels is not None:\n",
    "            # Calculate the loss with the labels provided\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(outputs[0].view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.gpt.generate(input_ids=input_ids, max_length=length)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Load the pre-trained GPT-2 model\n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Create an instance of ModifiedGPT with the required components\n",
    "    model = ModifiedGPT(\"gpt2\")\n",
    "\n",
    "    # Load the fine-tuned model weights\n",
    "    model.load_state_dict(torch.load(\"modified_gpt_model.pth\"))\n",
    "\n",
    "    prompt = \"egypt is\"\n",
    "    generated_text = generate_text(prompt, model, tokenizer, length=50)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0274a-f684-4d20-891b-4eeb66efa292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
